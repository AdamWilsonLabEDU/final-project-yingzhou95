---
title: "Identify characteristics that contribute to environmental safety based on people’s perception"
author: Ying Zhou
subtitle: The Case Study of New York City
date: today
date-format: long
---

# 1. Introduction

One goal of creating livable cities is to enhance health and safety. While previous research in spatial analysis and urban planning has focused on correlations between physical environments and crime, typically relying on police-reported crime data from sources like the Crime Open Database (CODE), safety perception is inherently subjective and cannot be fully represented by objective crime statistics alone. Also, urban planning today has gradually shifted its focus from a top-down mechanism to a bottom-up mechanism, so understanding and fostering spaces where residents feel safe is essential. This project explores spatial autocorrelation of the reported crime events and people perceived dangerous places; also, it examines characteristics of urban space that contribute to residents’ perceived insecurity in New York City.

# 2. Materials and methods

In addition to spatial analysis of the open crime data, the research utilizes social media data to acquire people’s perceptions. By adopting Latent Dirichlet Allocation (LDA), a method of topic modeling, the research filtered and summarized posted texts and contents related to the negative descriptions of places or spaces in the city, and then it identified the related characteristics of the environments. The characteristics are investigated by the method of local Moran’s I, which indicates their spatial autocorrelation in some neighborhoods in New York City.

## 2.1 Load Packages

```{r}
library(textcat)
library(dplyr)
library(stringr)
library(sf)
library(ggplot2)
library(spData)
library(crimedata)
library(tm)
library(textclean)
library(SnowballC)
library(foreach)
library(doParallel)
```

## 2.1 Data

Reported crime events: The Crime Open Database (CODE)

```{r}
library(crimedata)
# get crime data in NYC in 2017
crime_nyc_2017 <- get_crime_data(year = 2017, cities = "New York",
                                 type = "core", output = "sf")
```

People’s Perception: Twitter in 2017

```{r}
# load NYC twitter data in 2017
Twitter_path <- file.path("data", "nyc_twitterIn2017.csv")
nyc_twitterIn2017 <- read.csv(Twitter_path,
                                stringsAsFactors = FALSE)
```

### Clean the data
Warning: The following steps will take more than 7 days because the original twitter data is too big. If you want to do analysis in your computer, please load the cleaned data in the date file.Here, I just show the steps for datacleaning and sentiment analysis.
\# clean data
nyc_twitterIn2017 <- nyc_twitterIn2017 %>%
  select(text, created_at, lon, lat)
nyc_twitterIn2017$text <- gsub("http\\S+|www\\S+", "", nyc_twitterIn2017$text)
nyc_twitterIn2017$text <- gsub("@\\w+", "", nyc_twitterIn2017$text)
nyc_twitterIn2017$text <- tolower(nyc_twitterIn2017$text) # convert to lowercase text
nyc_twitterIn2017_clean <- nyc_twitterIn2017 %>%
  filter(textcat(text) == "english") %>% #select English tweets
  mutate(text = wordStem(text, language = "en")) #stemming
\# save the file for analysis
output_path <- file.path("data", "nyc_twitterIn2017_clean.csv")
write.csv(nyc_twitterIn2017, output_path, row.names = FALSE)

\# remove stop words
registerDoParallel(4)
getDoParWorkers()
custom_stopwords <- c(stopwords("en"), "with")
nyc_twitterIn2017_clean$text <- foreach(text = nyc_twitterIn2017_clean$text,
                                        .combine = c,
                                        .packages = "tm") %dopar% {
                                          removeWords(text, custom_stopwords)
                                        }
output_path <- file.path("data", "nyc_twitterIn2017_SWclean.csv")
write.csv(nyc_twitterIn2017_clean, output_path, row.names = FALSE)


### Sentiment Analysis

\# loading sentiment words
positive_path <- file.path("data", "positive-words.txt")
negative_path <- file.path("data", "negative-words.txt")
positive = scan(positive_path,
                what = 'character', comment.char = ';')
negative = scan(negative_path,
                what = 'character', comment.char = ';')
escaped_positive <- str_escape(positive)
escaped_negative <- str_escape(negative)

\# sentiment scoring function
sentiment_score <- function(text, positive, negative) {
  pos_count <- sum(str_detect(text, paste(positive, collapse = "|")))
  neg_count <- sum(str_detect(text, paste(negative, collapse = "|")))
  score <- pos_count - neg_count
  return(score)
}

\# sentiment score
nyc_twitterIn2017_score <- nyc_twitterIn2017_clean %>%
  rowwise() %>%
  mutate(sentiment = sentiment_score(text, escaped_positive, escaped_negative)) %>% 
  mutate(sentiment_label = case_when(                     
    sentiment > 0 ~ "positive",
    sentiment < 0 ~ "negative",
    TRUE ~ "neutral"
  ))



# Results

\[\~200 words\]

Tables and figures (maps and other graphics) are carefully planned to convey the results of your analysis. Intense exploration and evidence of many trials and failures. The author looked at the data in many different ways before coming to the final presentation of the data.

Show tables, plots, etc. and describe them.


### Dygraphs Example

# Conclusions

\[\~200 words\]

Clear summary adequately describing the results and putting them in context. Discussion of further questions and ways to continue investigation.

# References

All sources are cited in a consistent manner
